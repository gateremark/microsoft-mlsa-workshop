{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering challenge\n",
        "\n",
        "Clustering is an unsupervised machine learning technique, in which you train a model to group similar entities into clusters based on their features.\n",
        "\n",
        "In this exercise, you must separate a dataset consisting of three numeric features (*A*, *B*, and *C*) into clusters.\n",
        "\n",
        "Let's begin by importing the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Load the core tidyverse and make it available in your current R session\n",
        "suppressPackageStartupMessages(\n",
        "  library(tidyverse))\n",
        "\n",
        "# Initialize github repo path \n",
        "# containing test files used to check your answers\n",
        "testsFolderPath <- \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-machine-learning-with-r/main/tests/introduction-clustering-models/\"\n",
        "\n",
        "# Read the csv file into a tibble\n",
        "clust_data <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/challenges/data/clusters.csv\", show_col_types = FALSE)\n",
        "\n",
        "# Print the first 10 rows of the data\n",
        "clust_data %>% \n",
        "  slice_head(n = 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your challenge is to identify the number of discrete clusters present in the data, and create a clustering model that separates the data into that number of clusters. You should also visualize the clusters to evaluate the level of separation achieved by your model.\n",
        "\n",
        "Add markdown and code cells as required to create your solution.\n",
        "\n",
        "### Use PCA to create a 2D version of the features for visualization\n",
        "\n",
        "*Principal component analysis* (PCA) is a dimension reduction method that attempts to reduce the feature space. With PCA, most of the information or variability in the dataset can be explained by using fewer uncorrelated features. Let's see this in action by creating a specification of a `recipe` that estimates the principal components, based on our three variables. You'll later `prep` and `bake` the recipe to apply the computations.\n",
        "\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "For this question, create a specification of a `recipe` that will:\n",
        "- Normalize all predictors.\n",
        "- Convert all predictors into *two* principal components.\n",
        "\n",
        "Fill in the placeholder `....` with the right code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# Load the core tidymodels packages and make them available in your current R session\n",
        "suppressPackageStartupMessages(\n",
        "  library(tidymodels))\n",
        "\n",
        "# Specify a recipe\n",
        "pca_rec <- recipe(...., data = clust_data) %>%\n",
        "  # Recipe step for normalizing predictors\n",
        "  ....(all_predictors()) %>% \n",
        "  # Recipe step for converting all predictors into 2 Principal Components\n",
        "  ....(...., num_comp = ...., id = \"pca\")\n",
        "\n",
        "# Print out recipe\n",
        "pca_rec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your answer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "testFilePath <- paste(testsFolderPath, \"Question%201.R\", sep=\"\", collapse=NULL)\n",
        ". <- ottr::check(testFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well done! Compared to supervised learning techniques, this technique has no outcome variable in this recipe.\n",
        "\n",
        "Now, time to prep and bake the recipe!\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "In this section:\n",
        "- Prep the recipe you created above. Prepping a recipe estimates the required quantities and statistics required by PCA.\n",
        "- Bake the prepped recipe. This takes the trained (prepped) recipe, and applies its operations to a dataset to create a design matrix. \n",
        "\n",
        "Fill in the placeholder `....` with the right code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Estimate the recipe \n",
        "pca_estimates <- prep(....)\n",
        "\n",
        "# Return preprocessed clust_data using bake\n",
        "features_2d <- pca_estimates %>% \n",
        "  ....(new_data = NULL)\n",
        "\n",
        "# Print baked data set\n",
        "features_2d %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "testFilePath <- paste(testsFolderPath, \"Question%202.R\", sep=\"\", collapse=NULL)\n",
        ". <- ottr::check(testFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good job! After prep and bake, you obtain two components that capture the maximum amount of information (that is, variance) in the original variables.\n",
        "\n",
        "From the output of your prepped recipe `pca_estimates`, you can examine how much variance each component accounts for:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Examine how much variance each PC accounts for\n",
        "pca_estimates %>% \n",
        "  tidy(id = \"pca\", type = \"variance\") %>% \n",
        "  filter(str_detect(terms, \"percent\"))\n",
        "\n",
        "\n",
        "theme_set(theme_light())\n",
        "# Plot how much variance each PC accounts for\n",
        "pca_estimates %>% \n",
        "  tidy(id = \"pca\", type = \"variance\") %>% \n",
        "  filter(terms == \"percent variance\") %>% \n",
        "  ggplot(mapping = aes(x = component, y = value)) +\n",
        "  geom_col(fill = \"midnightblue\", alpha = 0.7) +\n",
        "  ylab(\"% of total variance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output's tibbles and plots show how well each principal component is explaining the original six variables. For example, the first principal component (PC1) explains about 84.93 percent of the variance of the three variables. The second principal component explains an additional 7.63 percent, giving a cumulative percent variance of 92.56 percent. This is certainly better. It means that the first two variables seem to have some power in summarizing the original three variables.\n",
        "\n",
        "Now that you have the data points translated to two dimensions PC1 and PC2, you can visualize them in a plot:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize PC scores\n",
        "features_2d %>% \n",
        "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
        "  geom_point(size = 2, color = \"dodgerblue3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate WCSS for multiple cluster numbers to determine the right number of clusters\n",
        "\n",
        "Here's one of the fundamental problems with clustering: without knowing class labels, how do you know how many clusters to separate your data into?\n",
        "\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "In this section, you'll try to find the optimal number of clusters that your data can be divided into. You do this by using your data sample, `clust_data`, to create a series of clustering models with an incrementing number of clusters. Then you measure how tightly the data points are grouped within each cluster.\n",
        "\n",
        "A metric often used to measure this tightness is the *within cluster sum of squares* (WCSS), with lower values meaning that the data points are closer. You can then plot the WCSS for each model.\n",
        "\n",
        "- Prep and bake `clust_data` to obtain a new tibble, where all the predictors are normalized.\n",
        "- Use the built-in `kmeans()` function to perform clustering, by creating a series of 10 clustering models.\n",
        "- Extract the WCSS, `tot.withinss`, for each cluster.\n",
        "\n",
        "Fill in the placeholder `....` with the right code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Obtain normalized data for estimating clusters\n",
        "clust_estimate_data <- recipe(~ ., data = clust_data) %>%\n",
        "  # Step to normalize all predictors\n",
        "  ....(all_predictors()) %>% \n",
        "  # Prep the recipe\n",
        "  .... %>% \n",
        "  # Apply trained recipe to train set: clust_data\n",
        "  ....\n",
        "\n",
        "# Print out data\n",
        "clust_estimate_data %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's explore the WCSS of different numbers of clusters.\n",
        "\n",
        "You can use `map()` from the [purrr](https://purrr.tidyverse.org/) package to apply functions to each element in list.\n",
        "\n",
        "> [`map()`](https://purrr.tidyverse.org/reference/map.html) functions allow you to replace many `for` loops with code that is both more succinct and easier to read. The best place to learn about the `map()` functions is the [iteration chapter](http://r4ds.had.co.nz/iteration.html) in R for data science.\n",
        ">\n",
        "> The function `broom::glance.kmeans()` accepts a model object, and returns a tibble with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# Create 10 models with 1 to 10 clusters\n",
        "kclusts <- tibble(k = ....) %>% \n",
        "  mutate(\n",
        "    # Create a series of clustering models with centers k\n",
        "    model = map(k, ~ kmeans(x = clust_estimate_data, centers = .x, nstart = 20)),\n",
        "    # Extract the Total WCSS tot.withinss, for each model\n",
        "    tot.withinss = map_dbl(...., ~ glance(.x)$tot.withinss))\n",
        "\n",
        "\n",
        "\n",
        "# Plot Total within-cluster sum of squares (tot.withinss) for each model with center k\n",
        "kclusts %>% \n",
        "  ggplot(mapping = aes(x = k, y = ....)) +\n",
        "  geom_line(size = 1.2, alpha = 0.5, color = \"dodgerblue3\") +\n",
        "  geom_point(size = 2, color = \"dodgerblue3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "testFilePath <- paste(testsFolderPath, \"Question%203.R\", sep=\"\", collapse=NULL)\n",
        ". <- ottr::check(testFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That went well! What do you think the chart is telling you?\n",
        "\n",
        "\n",
        "\n",
        "You want to minimize the total within-cluster sum of squares, by performing K-means clustering. The plot shows a large reduction in WCSS (so, greater tightness) as the number of clusters increases from one to two. The plot shows a further noticeable reduction from two to three, and then three to four clusters. After that, the reduction is less pronounced, resulting in an elbow in the chart at around four clusters. This is a good indication that there are four reasonably well-separated clusters of data points.\n",
        "\n",
        "### Use K-means\n",
        "\n",
        "**Question 4**\n",
        "\n",
        "\n",
        "In the previous section, you noticed that there are four reasonably well-separated clusters of data points. In this section:\n",
        "\n",
        "Extract the model that separates the data into k = 4 clusters. Starting with the clustering results `kclusts`:\n",
        "1. Create a subset where the models contains `k = 4` clusters.\n",
        "1. Use `pull()` to extract the clustering model in the `model` column.\n",
        "1. Retrieve the model components, such as cluster assignments, by using `pluck()`.\n",
        "\n",
        "Fill in the placeholder `....` with the right code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Extract the model with k = 4 clusters\n",
        "final_kmeans <- kclusts %>% \n",
        "  # Narrow down to the model with K = 4 centers\n",
        "  filter(....) %>%\n",
        "  # Extract the clustering model\n",
        "  ....(model) %>% \n",
        "  # Retrieve model components\n",
        "  ....(1)\n",
        "\n",
        "# Print model components\n",
        "final_kmeans\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "testFilePath <- paste(testsFolderPath, \"Question%204.R\", sep=\"\", collapse=NULL)\n",
        ". <- ottr::check(testFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What can you make out of the K-means object, `final_kmeans`? One of the available components is the *clustering vector*: a vector of integers (from 1 to k) indicating the cluster to which each observation in your data is allocated. Be sure to explore the other available components.\n",
        "\n",
        "\n",
        "### Plot the clustered points\n",
        "\n",
        "Great! Let's go ahead and visualize the clusters obtained.\n",
        "\n",
        "**Question 5**\n",
        "\n",
        "In this section:\n",
        "\n",
        "- Add the cluster assignments generated by the model to the 2D version (2 principal components) of your dataset. That is, you're adding the assignments to `features_2d`. Hint: `augment` accepts a model object and a dataset, and adds information about each observation in the dataset.\n",
        "\n",
        "- Visualize the cluster assignments by using a scatter plot. Assign the `shape` and `color` aesthetics to the cluster number assigned to each data point.\n",
        "\n",
        "Fill in the placeholder `....` with the right code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Load plotting libraries \n",
        "suppressPackageStartupMessages({\n",
        "  library(plotly)\n",
        "  library(paletteer)\n",
        "})\n",
        "\n",
        "\n",
        "# Augment featured_2d with information from the clustering model\n",
        "results <-  augment(...., ....) \n",
        "\n",
        "# Print some rows of the results\n",
        "results %>% \n",
        "  slice_head(n = 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voila! There go the two principal components (PC1 and PC2), which capture the maximum amount of information in the original variables, along with their corresponding cluster assignment (*.cluster*).\n",
        "\n",
        "Now, let's see this in an interactive plot!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Plot cluster assignments\n",
        "cluster_plot <- results %>% \n",
        "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
        "  geom_point(aes(...., ....), size = 2, alpha = 0.8) +\n",
        "  paletteer::scale_color_paletteer_d(\"ggsci::category10_d3\")\n",
        "\n",
        "# Make plot interactive\n",
        "ggplotly(cluster_plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "testFilePath <- paste(testsFolderPath, \"Question%205.R\", sep=\"\", collapse=NULL)\n",
        ". <- ottr::check(testFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully you're able to observe that the data has been separated into four distinct clusters. \n",
        "\n",
        "K-means is a commonly used clustering algorithm that separates a dataset into K clusters of equal variance. Observations within the same cluster are as similar as possible (that is, they have high intra-class similarity), whereas observations from different clusters are as dissimilar as possible (they have low inter-class similarity).\n",
        "\n",
        "The first step in K-means clustering is the data scientist specifying the number of clusters K to partition the observations into. Hierarchical clustering is an alternative approach, which doesn't require the number of clusters to be defined in advance. Let's jump right into it.\n",
        "\n",
        "### Try agglomerative clustering\n",
        "\n",
        "Hierarchical clustering creates clusters by either a *divisive* method or an *agglomerative* method. The divisive method is a top-down approach, starting with the entire dataset, and then finding partitions in a stepwise manner. Agglomerative clustering is a bottom-up approach, in which each observation is initially considered as a single element cluster. At each step of the algorithm, two clusters that are most similar are combined into a bigger cluster, until all points are members of one single, big cluster.\n",
        "\n",
        "\n",
        "**Question 6**\n",
        "\n",
        "In this section, you'll cluster the data by using an agglomerative clustering algorithm. There are many functions available in R for hierarchical clustering.\n",
        "\n",
        "The `hclust()` function is one way to perform hierarchical clustering. It requires a distance matrix structure, and a specification of the agglomeration method to be used. \n",
        "\n",
        "- Use the `dist` function to compute the distance matrix (distances between the rows of the data), by using the Euclidean method.\n",
        "\n",
        "- Use `hclust` to perform hierarchical clustering with the `ward.D2` (Ward linkage) agglomeration method.\n",
        "\n",
        "Fill in the placeholder `....` with the right code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# For reproducibility\n",
        "set.seed(2056)\n",
        "\n",
        "# Distance between observations matrix\n",
        "d <- ....(x = clust_estimate_data, method = \"....\")\n",
        "\n",
        "# Hierarchical clustering using Ward Linkage\n",
        "hclust_ward <- ....(d, method = \"....\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "testFilePath <- paste(testsFolderPath, \"Question%206.R\", sep=\"\", collapse=NULL)\n",
        ". <- ottr::check(testFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [factoextra](https://rpkgs.datanovia.com/factoextra/index.html) provides functions ([`fviz_dend()`](https://rdrr.io/pkg/factoextra/man/fviz_dend.html)) to visualize hierarchical clustering. Let's visualize the dendrogram representation of the clusters from the Ward linkage method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "library(factoextra)\n",
        "\n",
        "# Visualize cluster separations\n",
        "fviz_dend(hclust_ward, main = \"Ward Linkage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although hierarchical clustering doesn't require you to pre-specify the number of clusters, you still need to specify the number of clusters to extract.\n",
        "\n",
        "You found out, by using the `WCSS` method, that the optimal number of clusters was 4. Let's color our dendrogram according to k = 4, and observe how observations will be grouped.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize clustering structure for 4 groups\n",
        "fviz_dend(hclust_ward, k = 4, main = \"Ward Linkage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect! You can now go ahead and cut the hierarchical clustering model into four clusters, and extract the cluster labels for each observation associated with a particular cut. This is done by using `cutree()`.\n",
        "\n",
        "**Question 7**\n",
        "\n",
        "In this section, you cut the hierarchical clustering model into four clusters, and extract the cluster labels for each observation.\n",
        "\n",
        "Starting with the principal components version of the data:\n",
        "\n",
        "- Using the `cutree` function, extract k = 4 groups of data obtained from the hierarchical clustering process. Assign this to a column name `cluster_id`.\n",
        "\n",
        "- Also, encode the `cluster_id` column as a factor.\n",
        "\n",
        "Fill in the placeholder `....` with the right code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# Extract cluster labels obtained from the Hierarchical clustering process\n",
        "results_hclust <- features_2d %>% \n",
        "  ....(cluster_id = ....)\n",
        "\n",
        "\n",
        "# Print some rows of the results\n",
        "results_hclust %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "testFilePath <- paste(testsFolderPath, \"Question%207.R\", sep=\"\", collapse=NULL)\n",
        ". <- ottr::check(testFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! Now, let's inspect the cluster assignments visually.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### View the agglomerative cluster assignments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Plot h-cluster assignment on the PC data\n",
        "hclust_spc_plot <- results_hclust %>% \n",
        "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
        "  geom_point(aes(shape = cluster_id, color = cluster_id), size = 2, alpha = 0.8) +\n",
        "  paletteer::scale_color_paletteer_d(\"ggsci::category10_d3\")\n",
        "\n",
        "# Make plot interactive\n",
        "ggplotly(hclust_spc_plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How does this compare to the K-means results you visualized earlier?\n",
        "\n",
        "You can see that the observations were grouped quite similarly by the two clustering algorithms, K-means and hierarchical. \n",
        "\n",
        "We'll wrap it at that! In this challenge, you created unsupervised machine learning models, to help you group unlabeled data observations into clusters. \n",
        "\n",
        "Congratulations on this amazing feat!\n",
        "\n",
        "Happy Learning,\n",
        "\n",
        "[Eric](https://twitter.com/ericntay), Gold Microsoft Learn Student Ambassador.\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "langauge": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
