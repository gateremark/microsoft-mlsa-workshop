{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID Paper Exploration Workshop\n",
    "\n",
    "This workshop explores the use of [Azure Text Analytics](https://azure.microsoft.com/services/cognitive-services/text-analytics/?WT.mc_id=academic-49822-dmitryso) and [Text Analytics for Health](https://docs.microsoft.com/azure/cognitive-services/language-service/text-analytics-for-health/overview/?WT.mc_id=academic-49822-dmitryso) to get some insights from a large corpus of COVID medical papers. Full version of the workshop is available [here](http://eazify.net/paper-exploraton-workshop). \n",
    "\n",
    "Let's start by importing some of the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import bz2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Milestone 1: Getting the Dataset\n",
    "\n",
    "In our example, we will only analyze a subset of paper abstracts. All paper metadata is available in **Metadata file** [Metadata.csv](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge?select=metadata.csv). You need to:\n",
    "\n",
    "1. Download [metadata.csv](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge?select=metadata.csv) file from Kaggle (you may need to log in / register first)\n",
    "1. Place it in the `data` directory and unzip if needed.\n",
    "\n",
    "> Notice that `metadata.csv` file is rather large, more than 1 Gb. This may cause file processing to be slow at some times. You should not be surprised, as we are dealing with rather large amounts of real world data!\n",
    "\n",
    "We can then use Pandas library to load the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the code to load and display the first rows of the dataset.\n",
    "\n",
    "# If you are short on time, you can also load the old version of the dataset directly over the network:\n",
    "# df = pd.read_csv(\"https://datascience4beginners.blob.core.windows.net/cord/metadata.csv.zip\",compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** is a very frequently used Python library to manipulate tabular data. You can [read more](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/2-Working-With-Data/07-python) about using Pandas for data processing in our [Data Science for Beginners Curriculum](https://github.com/microsoft/Data-Science-For-Beginners/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2: Initial Data Cleanup\n",
    "\n",
    "Once we have the data, you need to do some data cleanup. Make sure to do the following:\n",
    "\n",
    "* Convert `publication_time` column to datetime\n",
    "* Filter out all publication times outside 2020-current time\n",
    "\n",
    "After cleanup, you may want to plot the histogram of # of publications by date (eg. monthly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3: Creating and Using Text Analytics Endpoint\n",
    "\n",
    "At this point, you should have your Azure subscription ready. Start by logging into [Azure Portal](http://portal.azure.com/?WT.mc_id=academic-49822-dmitryso).\n",
    "\n",
    "Then, create [Azure Cognitive Service for Language](https://docs.microsoft.com/azure/cognitive-services/language-service/overview/?WT.mc_id=academic-49822-dmitryso) cloud resource. You can start creating the resource by clicking [**HERE**](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics) - it will take you to the corresponding page on the Azure Portal.\n",
    "\n",
    "> Make sure to select **S - Standard** pricing tier, because Health Analytics is not available under the Free Tier.\n",
    "\n",
    "Once you have create the resource, you should go to the portal and copy **Endpoint URL** and **Access key**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'place URL here'\n",
    "key = 'place key here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the Cognitive Service for Language we need to install corresponding Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install azure.ai.textanalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an object to call the service, and test it on a couple of sample abstracts. On Microsoft Docs, you can find the [documentation and sample code](https://docs.microsoft.com/azure/cognitive-services/language-service/text-analytics-for-health/quickstart?WT.mc_id=academic-49822-dmitryso&pivots=programming-language-python#code-example). As a result, print all entities and relations found in the abstract, with their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 4: Processing Abstracts\n",
    "\n",
    "Now it's time to go big and process abstracts at scale! However, because we are limited in time, and we do not want to waste your cloud resources, we will process only a limited number of random abstracts (say, 200-500). \n",
    "\n",
    "Keep in mind that some abstracts are not present (they will have 'NaN' )\n",
    "\n",
    "> It is important to select abstracts randomly, because later on we will want to explore the change is treatment tactics over time, and we need to have uniform paper representation across all time period. Alternatively, to further minimize time/spend, you can select a time sub-interval (say, only year 2020), and then process random papers in that interval. \n",
    "\n",
    "Spend some time thinking about the way you will store the result of processing. You can add processing results as additional columns to the DataFrame, or you can use separate list/dictionary.\n",
    "\n",
    "> You want to make sure that for each paper you keep essential info such as title and publication time, together with all entities and relations.\n",
    "\n",
    "Processing can take quite a long time. You may start (and proceed until the end of the workshop) with small sample size (~50 papers) to make sure your code works and your data structure is right, and then increase the sample size to 200-500 towards the end to obtain the results. \n",
    "\n",
    "> **NOTE**: If this assignment seems to difficult, or you do not have enough time to process sufficiently many abstracts - you can go to the next cell to load up pre-processed results that we have put together for you. However, designing your own storage format and doing this yourself is a great exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the code here\n",
    "# You can store the results into a variable named store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you do not want to wait until processing all data, you can open our pre-processed collection of papers right away using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('..\\data\\processed.pkl.bz2','r') as f:\n",
    "    store = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 5: Get Top Symptoms, Medications and Diagnoses\n",
    "\n",
    "To simplify further data processing, let's build the DataFrame of all entities. We shall go through all papers and populate a DataFrame as a result.\n",
    "\n",
    "> Each entity has links to **ontologies**, which are structured hierarchies of medical terms. For example, [Unified Medical Language System](https://en.wikipedia.org/wiki/Unified_Medical_Language_System), or UMLS, contains codes for all medications, diagnoses, treatments, etc. Several terms, such as **SARS-Cov-2** and **COVID-19** can refer to the same entity, so to account for that we shall also extract and store UMLS ontology ID for each entity. \n",
    "\n",
    "Things to do:\n",
    "* Create a function that will get UMLS Ontology ID for an entity\n",
    "* Populate a DataFrame with all enitites and corresponding publications/publication_time's\n",
    "* Compute most frequently mentioned entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of interest, let's build the word cloud of all diagnoses. You can use [this tutorial](https://www.datacamp.com/community/tutorials/wordcloud-python) as a guideline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 6: Visualize Change in Treatment Strategies\n",
    "\n",
    "In addition to calculating total count of mentions, you can see how they are distributed by month, and this detect changes in treatment strategies. To do this:\n",
    "\n",
    "* Select top 5-10 medications/medication classes (AKA treatment strategies)\n",
    "* Compute the distribution of their mentions by months (or weeks)\n",
    "* Plot monthly distribution of mentions either on separate graphs, or on a bar plot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 7: Visualize Co-occurrence of Terms\n",
    "\n",
    "It is interesting to see which terms occur together within one paper, because it can give us an idea about relationships between, for example, diagnoses and medications, or symptoms and treatments. You should also be able to see which medications are often used together, and which symptoms occur together.\n",
    "\n",
    "You can use two types of diagrams for that:\n",
    "\n",
    "* **Sankey diagram** allows us to investigate relations between two types of terms, eg. diagnosis and treatment\n",
    "* **Chord diagram** helps to visualize co-occurrence of terms of the same type (eg. which medications are mentioned together)\n",
    "\n",
    "To plot both diagrams, we need to compute co-occurrence matrix, which in the row i and column j contains number of co-occurrences of terms i and j in the same abstract (one can notice that this matrix is symmetric).\n",
    "\n",
    "What to do:\n",
    "* Use the ontologies defined below or create your own ones\n",
    "* Define a function to compute co-occurence matrix for two given ontologies\n",
    "* Plot this matrix either using sankey diagram, or using chord diagram (in which case you will need to call your function with the same ontology twice) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medclass_ontology = {\n",
    "    'C0003451' : ('antivirals',0),\n",
    "    'C0003232' : ('antibiotics',1),\n",
    "    'C0001617' : ('corticosteroid',2),\n",
    "    'C0038317' : ('steroids',2),\n",
    "    'C0003280' : ('anticoagulation',3) }\n",
    "\n",
    "diag_ontology = {\n",
    "    'C5203670' : ('Covid-19',0),\n",
    "    'C3714514' : ('infection',1),\n",
    "    'C0011065' : ('death',2),\n",
    "    'C0042769' : ('virus',3),\n",
    "    'C0206750' : ('coronavirus',3),\n",
    "    'C0026565' : ('mortality',2),\n",
    "    'C0006826' : ('cancer',4),\n",
    "    'C5203676' : ('SARS',5) }\n",
    "\n",
    "med_ontology = {\n",
    "    'C0020336' : ('hydroxychloroquine',0),\n",
    "    'C0008269' : ('chloroquine',0),\n",
    "    'C0939237' : ('lopinavir + ritonavir',1),\n",
    "    'C4726677' : ('Remdesivir',2),\n",
    "    'C1609165' : ('tocilizumab',3),\n",
    "    'C0052796' : ('azithromycin',4),\n",
    "    'C0292818' : ('ritonavir',5) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To actually plot the diagrams, we can use [Plotly](https://plotly.com/python/) graphics library. This process is well described [here](https://plotly.com/python/sankey-diagram/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a code to install and then import Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a code to produce sankey diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot chord diagram, you can use [Holoviews](https://holoviews.org/) library. You may need to install it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c pyviz holoviews bokeh\n",
    "\n",
    "import holoviews as hv\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a code to produce the following chord diagram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next\n",
    "\n",
    "Now that you have learnt how you can use knowledge extraction from text, you can try to apply this to different problems. For different knowledge domains, however, you would need to train your own NER neural network model, and for that you will also need a dataset. [Custom Named Entity Recognition](https://docs.microsoft.com/azure/cognitive-services/language-service/custom-named-entity-recognition/overview/?WT.mc_id=academic-49822-dmitryso) service can help you do that. However, [Text Analytics Service](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/?WT.mc_id=academic-49822-dmitryso) also has some [pre-built entity extraction mechanism](https://docs.microsoft.com/azure/cognitive-services/language-service/named-entity-recognition/concepts/named-entity-categories/?WT.mc_id=academic-49822-dmitryso), as well as keyword extraction.\n",
    "\n",
    "Some of the projects you may want to build:\n",
    "\n",
    "* Analyze a blog or social network posts and get the idea of different topics that author is writing about. See how interests change over time, as well as the mood. You can use the blog of [Scott Hanselman](https://www.hanselman.com/), it goes back to [2002](https://www.hanselman.com/blog/archive/2002).\n",
    "* Analyze [COVID 19 twitter feed](https://github.com/thepanacealab/covid19_twitter) to see if you can extract changes in major topics on twitter.\n",
    "* Analyze your e-mail archive to see how the topics you discuss and your mood change over time. Most e-mail clients allow you to export your e-mails to plain text or CSV format (here is an [example for Outlook](https://support.microsoft.com/office/import-and-export-outlook-email-contacts-and-calendar-92577192-3881-4502-b79d-c3bbada6c8ef/?WT.mc_id=academic-49822-dmitryso)).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86193a1ab0ba47eac1c69c1756090baa3b420b3eea7d4aafab8b85f8b312f0c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('azureml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
